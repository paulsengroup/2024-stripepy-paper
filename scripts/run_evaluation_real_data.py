#!/usr/bin/env python3

# Copyright (C) 2024 Andrea Raffo <andrea.raffo@ibv.uio.no>
#
# SPDX-License-Identifier: MIT


import argparse
import pathlib

import numpy as np
import pandas as pd
from sklearn.metrics import confusion_matrix
from utils import IO, evaluate

# Resolution
resolution = 10000

# Output path:
output_path = pathlib.Path("./output/real data/tables/")

# Times measured using the terminal, here stored in a list for table generation:
wall_clock_times = {
    "4DNFI9GMP2J8.fixed": ["2m7s", "9m17s", "11m26s", "1h9m55s"],
    "4DNFI6HDY7WZ.fixed": ["5m7s", "11m24s", "15m21s", "1h52m16s"],
    "ENCFF993FGR": ["3m20s", "6m21s", "4m28s", "1h19m14s"],
    "ENCFF216QQM.fixed": ["3m34s", "26m53s", "4m29s", "42m8s"],
}


def parse_args():
    class CustomFormatter(argparse.RawTextHelpFormatter):
        def _fill_text(self, text, width, indent):
            return "".join([indent + line + "\n" for line in text.splitlines()])

    def _existing_file(arg: str) -> pathlib.Path:
        print(arg)
        if (path := pathlib.Path(arg)).is_file():
            return path

        raise FileNotFoundError(arg)

    def _existing_path(arg):
        path = pathlib.Path(arg).parent
        if path.exists() and path.is_dir():
            return arg

        raise FileNotFoundError(f"Path not reachable: {path}")

    cli = argparse.ArgumentParser(
        description="Routine to evaluate StripePy, Chromosight, StripeCaller and Stripenn over real data. This "
        "script requires to download the following files and folders:\n"
        "• the real contact maps\n"
        "• the ChIP-seq peaks\n"
        "• StripePy's output folder\n"
        "• Chromosight's output folder\n"
        "• StripeCaller's output folder\n"
        "• Stripenn's output folder\n"
        "The location of the contact maps and ChIP-seq peaks are documented in the StripePy manuscript.\n"
        "The output folders of the stripe callers are here collected: https://zenodo.org/records/14449731.\n",
        formatter_class=CustomFormatter,
    )

    cli.add_argument(
        "contact-map",
        type=_existing_file,
        help="Path to an .mcool file for input (e.g., /tmp/ENCFF216QQM.fixed.mcool).",
    )

    cli.add_argument(
        "chip-seq-peaks",
        type=_existing_file,
        help="Path to the ChIP-seq peaks in .bed.gz format (e.g., /tmp/ENCFF796WRU.bed.gz).",
    )

    cli.add_argument(
        "--stripepy",
        type=_existing_path,
        required=True,
        help="Path to the folder containing the .hdf5 file generated by StripePy (e.g., /tmp/stripepy/real data/ENCFF216QQM.fixed).",
    )

    cli.add_argument(
        "--chromosight",
        type=_existing_path,
        required=True,
        help="Path to the folder containing the .tsv file generated by Chromosight (e.g., /tmp/chromosight/real data/ENCFF216QQM.fixed).",
    )

    cli.add_argument(
        "--stripecaller",
        type=_existing_path,
        required=True,
        help="Path to the folder containing the .BEDPE file generated by StripeCaller (e.g., /tmp/stripecaller/real data/ENCFF216QQM.fixed).",
    )

    cli.add_argument(
        "--stripenn",
        type=_existing_path,
        required=True,
        help="Path to the folder containing the .tsv file generated by Stripenn (e.g., /tmp/stripenn/real data/ENCFF216QQM.fixed).",
    )

    # Parse the input parameters:
    args = vars(cli.parse_args())

    # Gather input parameters in dictionaries:
    configs_input = {key: args[key] for key in ["contact-map", "chip-seq-peaks"]}
    configs_methods = {key: args[key] for key in ["stripepy", "chromosight", "stripecaller", "stripenn"]}

    # Print the used parameters (chosen or default-ones):
    print("\nArguments:")
    print(f"contact-map: {configs_input['contact-map']}")
    print(f"chip-seq: {configs_input['chip-seq-peaks']}")
    print(f"M1: {configs_methods['stripepy']}")
    print(f"M2: {configs_methods['chromosight']}")
    print(f"M3: {configs_methods['stripecaller']}")
    print(f"M4: {configs_methods['stripenn']}")

    return configs_input, configs_methods


if __name__ == "__main__":

    # Import parameters:
    configs_input, configs_methods = parse_args()

    # Contact map loading:
    c, chr_starts, chr_ends, chr_sizes = IO.cmap_loading(str(configs_input["contact-map"]), resolution)
    chrs = list(c.chromosomes().keys())
    chr_ids = list(range(len(chrs)))
    c_pairs = list(zip(chr_ids, chrs))

    # Prefixes:
    prefixes = ["M1", "M2", "M3", "M4"]  # M1 = stripepy, M2 =chromosight, M3 = stripecaller, M4 = stripenn

    # Ingredients for recognition measures:
    is_anchor_found = {prefix: [] for prefix in prefixes}  # 0-1 classification vector
    is_candidate_good = {prefix: [] for prefix in prefixes}  # 0-1 classification vector

    # Numbers of anchors/stripes:
    n_found_anchors = {prefix: 0 for prefix in prefixes}  # number of found anchor sites
    n_predicted_stripes = {prefix: 0 for prefix in prefixes}  # number of predicted candidate stripes

    # Classification and recognition measures:
    confusion_matrices = {prefix: {key: 0 for key in ["TP", "TN", "FP", "FN"]} for prefix in prefixes}
    metrics_keys = ["TPR", "TNR", "PPV", "bACC", "GM", "JI", "F1c", "FMc", "AHR", "FGC", "F1r", "FMr"]
    metrics = {prefix: {key: 0 for key in metrics_keys} for prefix in prefixes}

    # ChIP-seq peaks:
    header_names = ["chrom", "start", "end", "name", "score", "strand", "signalValue", "pValue", "qValue", "peak"]
    peaks = pd.read_csv(configs_input["chip-seq-peaks"], compression="gzip", sep="\t", header=None, names=header_names)
    peaks["anchor"] = (peaks["start"] + peaks["end"]) / 2
    peaks = peaks.astype({"anchor": int})

    # Loop over chromosomes (chrX, chrY, and chrM are not considered as overly sparse):
    print("\nAnalysis of results:")
    for chr_idx, chr in c_pairs[:-3]:

        print(f"-> {chr}")

        # Contact map for this chromosome:
        I = c.fetch(chr).to_coo().tocsr()

        # Peaks for this chromosome:
        these_peaks = peaks[peaks["chrom"] == chr]

        # Ground-truth classification vector:
        GT_anchors = np.unique(np.round(these_peaks.anchor.values / resolution)).astype(int).tolist()
        GT_anchors = np.unique(GT_anchors)
        GT_clas_vec = np.isin(np.arange(I.shape[0]), GT_anchors).astype(int)

        # --- Method M1 ---

        # Load M1 and computation:
        path = pathlib.Path(f"{configs_methods['stripepy']}/{resolution}/")
        HIoIs, _, pred_clas_vec, seeds = IO.retrieve_stripepy(path, chr, I.shape[0], 3.0)

        # Classification measures:
        pred_clas_vec = pred_clas_vec["LT"] + pred_clas_vec["UT"]
        pred_clas_vec = np.where(pred_clas_vec > 1, 1, pred_clas_vec)
        tn, fp, fn, tp = confusion_matrix(GT_clas_vec, pred_clas_vec, labels=[0, 1]).ravel()
        confusion_matrices["M1"]["TP"] += tp
        confusion_matrices["M1"]["TN"] += tn
        confusion_matrices["M1"]["FP"] += fp
        confusion_matrices["M1"]["FN"] += fn

        # Recognition measures:
        x = evaluate._is_anchor_in_stripes(
            GT_anchors=GT_anchors, pred_HIoIs=HIoIs["LT"].tolist() + HIoIs["UT"].tolist()
        )
        is_anchor_found["M1"] += x[0].tolist()
        is_candidate_good["M1"] += x[1].tolist()

        # Numbers:
        n_found_anchors["M1"] += np.sum(x[0])
        n_predicted_stripes["M1"] += HIoIs["LT"].shape[0] + HIoIs["UT"].shape[0]

        # --- Method M2 ---

        # Load M2 and computation:
        path = pathlib.Path(f"{configs_methods['chromosight']}/{resolution}/")
        HIoIs, _, pred_clas_vec, _ = IO.retrieve_chromosight(path, chr, I.shape[0], resolution)

        # Classification measures:
        pred_clas_vec = pred_clas_vec["LT"] + pred_clas_vec["UT"]
        pred_clas_vec = np.where(pred_clas_vec > 1, 1, pred_clas_vec)
        tn, fp, fn, tp = confusion_matrix(GT_clas_vec, pred_clas_vec, labels=[0, 1]).ravel()
        confusion_matrices["M2"]["TP"] += tp
        confusion_matrices["M2"]["TN"] += tn
        confusion_matrices["M2"]["FP"] += fp
        confusion_matrices["M2"]["FN"] += fn

        # Recognition measures:
        x = evaluate._is_anchor_in_stripes(
            GT_anchors=GT_anchors, pred_HIoIs=HIoIs["LT"].tolist() + HIoIs["UT"].tolist()
        )
        is_anchor_found["M2"] += x[0].tolist()
        is_candidate_good["M2"] += x[1].tolist()

        # Numbers:
        n_found_anchors["M2"] += np.sum(x[0])
        n_predicted_stripes["M2"] += HIoIs["LT"].shape[0] + HIoIs["UT"].shape[0]

        # --- Method M3 ---

        # Load M3 and computation:
        path = pathlib.Path(f"{configs_methods['stripecaller']}/{resolution}/")
        HIoIs, _, pred_clas_vec, _ = IO.retrieve_stripecaller(path, chr, I.shape[0], resolution)

        # Classification measures:
        pred_clas_vec = pred_clas_vec["LT"] + pred_clas_vec["UT"]
        pred_clas_vec = np.where(pred_clas_vec > 1, 1, pred_clas_vec)
        tn, fp, fn, tp = confusion_matrix(GT_clas_vec, pred_clas_vec, labels=[0, 1]).ravel()
        confusion_matrices["M3"]["TP"] += tp
        confusion_matrices["M3"]["TN"] += tn
        confusion_matrices["M3"]["FP"] += fp
        confusion_matrices["M3"]["FN"] += fn

        # Recognition measures:
        x = evaluate._is_anchor_in_stripes(
            GT_anchors=GT_anchors, pred_HIoIs=HIoIs["LT"].tolist() + HIoIs["UT"].tolist()
        )
        is_anchor_found["M3"] += x[0].tolist()
        is_candidate_good["M3"] += x[1].tolist()

        # Numbers:
        n_found_anchors["M3"] += np.sum(x[0])
        n_predicted_stripes["M3"] += HIoIs["LT"].shape[0] + HIoIs["UT"].shape[0]

        # --- Methods M4 ---

        # Load M4 and computation:
        path = pathlib.Path(f"{configs_methods['stripenn']}/{resolution}/")
        HIoIs, VIoIs, pred_clas_vec, _ = IO.retrieve_stripenn(path, chr, I.shape[0], resolution, filter_=True)

        # Classification measures:
        pred_clas_vec = pred_clas_vec["LT"] + pred_clas_vec["UT"]
        pred_clas_vec = np.where(pred_clas_vec > 1, 1, pred_clas_vec)
        tn, fp, fn, tp = confusion_matrix(GT_clas_vec, pred_clas_vec, labels=[0, 1]).ravel()
        confusion_matrices["M4"]["TP"] += tp
        confusion_matrices["M4"]["TN"] += tn
        confusion_matrices["M4"]["FP"] += fp
        confusion_matrices["M4"]["FN"] += fn

        # Recognition measures:
        x = evaluate._is_anchor_in_stripes(
            GT_anchors=GT_anchors, pred_HIoIs=HIoIs["LT"].tolist() + HIoIs["UT"].tolist()
        )
        is_anchor_found["M4"] += x[0].tolist()
        is_candidate_good["M4"] += x[1].tolist()

        # Numbers:
        n_found_anchors["M4"] += np.sum(x[0])
        n_predicted_stripes["M4"] += HIoIs["LT"].shape[0] + HIoIs["UT"].shape[0]

    # Compute the classification and recognition measures:
    evaluate.compute_classification_measures(confusion_matrices, metrics)
    evaluate.compute_recognition_measures(is_anchor_found, is_candidate_good, metrics)

    # Save table to CSV:
    path2output = pathlib.Path(f"{output_path}/table6-{configs_input['contact-map'].stem}.csv")
    IO.real_data_csv_measures(
        metrics, n_found_anchors, n_predicted_stripes, wall_clock_times[configs_input["contact-map"].stem], path2output
    )

    print(f"\nTABLE CONTAINING CLASSIFICATION AND RECOGNITION MEASURES...")
    print("Columns: measure - M1 - M2 - M3 - M4")
    IO.real_data_LaTex_table(metrics)
    print(
        f" & &  nAF & {n_found_anchors['M1']} & {n_found_anchors['M2']} & {n_found_anchors['M3']} & "
        f"{n_found_anchors['M4']} \\\\"
    )
    print(
        f" & & nSP & {n_predicted_stripes['M1']} & {n_predicted_stripes['M2']} & {n_predicted_stripes['M3']} & "
        f"{n_predicted_stripes['M4']} \\\\"
    )
    print("\\cdashline{3-7}")
    print("\\rule{0pt}{0.80\\normalbaselineskip}")
